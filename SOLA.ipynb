{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ZBx_2xq0rPtjfeh6ECTdlDze2YVXtMXO",
      "authorship_tag": "ABX9TyMz6FymU1JFtt8UEj3c6s+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sn715/SOLA/blob/main/SOLA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gzjO4tucfKyV"
      },
      "outputs": [],
      "source": [
        "#@title IMPORTS\n",
        "from music21 import *\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import *\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjUebmlybEwl",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title READ MIDI\n",
        "def read_midi(file):\n",
        "\n",
        "    print(\"Loading Music File:\",file)\n",
        "\n",
        "    notes=[]\n",
        "    notes_to_parse = None\n",
        "\n",
        "    #parsing a midi file\n",
        "    midi = converter.parse(file)\n",
        "\n",
        "    #group based on different instruments\n",
        "    s2 = instrument.partitionByInstrument(midi)\n",
        "\n",
        "    #loop over all the instruments\n",
        "    for part in s2.parts:\n",
        "\n",
        "        notes_to_parse = part.recurse()\n",
        "\n",
        "        #find whether a particular element is note or a chord\n",
        "        for element in notes_to_parse:\n",
        "\n",
        "            #note\n",
        "            if isinstance(element, note.Note):\n",
        "                notes.append(str(element.pitch))\n",
        "\n",
        "            #chord\n",
        "            elif isinstance(element, chord.Chord):\n",
        "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "\n",
        "    return np.array(notes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CLASSICAL CALM DATA\n",
        "\n",
        "def createClassicalCalmData(musicPath, emotionData):\n",
        "\n",
        "    #CSV EMOTION DATA\n",
        "\n",
        "    new_data = emotionData.groupby('track id').mean().round(0)\n",
        "    new_data['track_id'] = new_data.index\n",
        "    final_data = new_data[['track_id', 'calmness', 'power']]\n",
        "\n",
        "    calm_data_list = []\n",
        "\n",
        "    for x in final_data.index:\n",
        "        if final_data['calmness'][x] == 1.0:\n",
        "            calm_data_list.append(x)\n",
        "\n",
        "    calm_data = pd.DataFrame(calm_data_list)\n",
        "    calm_data.to_csv('final_calm_data_for_annotation.csv')\n",
        "\n",
        "    #MUSIC DATA\n",
        "\n",
        "    #read all the filenames\n",
        "    files=[i for i in os.listdir(musicPath) ]\n",
        "\n",
        "    calmFiles = []\n",
        "\n",
        "    for i in files:\n",
        "        if i[0:2] != \".i\":\n",
        "          if int(i[0:2]) in calm_data_list:\n",
        "            print(i)\n",
        "            calmFiles.append(i)\n",
        "\n",
        "    #reading each midi file\n",
        "    notes_array = np.array([read_midi(musicPath+i) for i in calmFiles])\n",
        "\n",
        "    #converting 2D array into 1D array\n",
        "    notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "    #No. of unique notes\n",
        "    unique_notes = list(set(notes_))\n",
        "\n",
        "    freq = dict(Counter(notes_))\n",
        "    frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "\n",
        "    #concatenating all audio files\n",
        "    new_music=[]\n",
        "\n",
        "    for notes in notes_array:\n",
        "        temp=[]\n",
        "        for note_ in notes:\n",
        "            if note_ in frequent_notes:\n",
        "                temp.append(note_)\n",
        "        new_music.append(temp)\n",
        "\n",
        "    new_music = np.array(new_music)\n",
        "\n",
        "    #cutting all files to 1000\n",
        "    diff = 0\n",
        "\n",
        "    for i in range(26):\n",
        "        diff = len(new_music[i]) - 1000\n",
        "        if (diff > 0):\n",
        "          new_music[i] = new_music[i][:-diff]\n",
        "        elif (diff < 0):\n",
        "          for j in range(-diff):\n",
        "            new_music[i].append(0)\n",
        "\n",
        "    list(set(new_music.ravel()[0]))\n",
        "\n",
        "    #DICTIONARY unique notes --> numbers\n",
        "    unique_x = np.unique(np.asarray([new_music[i][0:1001] for i in range(26)]).reshape(-1))\n",
        "    x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
        "\n",
        "    #preparing input sequences\n",
        "    x_seq=[]\n",
        "    for i in new_music:\n",
        "        temp=[]\n",
        "        for j in i:\n",
        "            #assigning unique integer to every note\n",
        "            temp.append(x_note_to_int[str(j)])\n",
        "        x_seq.append(temp)\n",
        "\n",
        "    x_seq = np.array(x_seq)\n",
        "\n",
        "    x = []\n",
        "    window_size = 32 #increase\n",
        "    for i in tqdm(x_seq):\n",
        "        this_x_ = [] # placeholder\n",
        "        for j in range(len(i)-window_size):\n",
        "            this_x_.append(i[j:(j+window_size)])\n",
        "        x.append(this_x_)\n",
        "\n",
        "    x_arr = np.asarray(x)\n",
        "    x_arr_resh = x_arr.reshape((26, 968, 32, 1))\n",
        "\n",
        "    bigX, bigY = x_arr_resh[:, 0:967, :, :], x_arr_resh[:, 967, :, :]\n",
        "    bigY = bigY.reshape((26, 32))\n",
        "\n",
        "    return bigX, bigY, calm_data, new_music\n"
      ],
      "metadata": {
        "id": "3m_tR9EfxrIq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CLASSICAL POWER DATA\n",
        "\n",
        "def createClassicalPowerData(musicPath, emotionData):\n",
        "\n",
        "    #CSV EMOTION DATA\n",
        "\n",
        "    new_data = emotionData.groupby('track id').mean().round(0)\n",
        "    new_data['track_id'] = new_data.index\n",
        "    final_data = new_data[['track_id', 'calmness', 'power']]\n",
        "\n",
        "    power_data_list = []\n",
        "\n",
        "    for x in final_data.index:\n",
        "        if final_data['power'][x] == 1.0:\n",
        "            power_data_list.append(x)\n",
        "\n",
        "    power_data = pd.DataFrame(power_data_list)\n",
        "    power_data.to_csv('final_power_data_for_annotation.csv')\n",
        "\n",
        "    #MUSIC DATA\n",
        "\n",
        "    #read all the filenames\n",
        "    files=[i for i in os.listdir(musicPath) ]\n",
        "\n",
        "    powerFiles = []\n",
        "\n",
        "    for i in files:\n",
        "        if i[0:2] != \".i\":\n",
        "          if int(i[0:2]) in power_data_list:\n",
        "            print(i)\n",
        "            powerFiles.append(i)\n",
        "\n",
        "    #reading each midi file\n",
        "    notes_array = np.array([read_midi(musicPath+i) for i in powerFiles])\n",
        "\n",
        "    #converting 2D array into 1D array\n",
        "    notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "    #No. of unique notes\n",
        "    unique_notes = list(set(notes_))\n",
        "\n",
        "    freq = dict(Counter(notes_))\n",
        "    frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "\n",
        "    #concatenating all 100 audio files\n",
        "    new_music=[]\n",
        "\n",
        "    for notes in notes_array:\n",
        "        temp=[]\n",
        "        for note_ in notes:\n",
        "            if note_ in frequent_notes:\n",
        "                temp.append(note_)\n",
        "        new_music.append(temp)\n",
        "\n",
        "    new_music = np.array(new_music)\n",
        "\n",
        "    #cutting all files to 1000\n",
        "    diff = 0\n",
        "\n",
        "    for i in range(26):\n",
        "        diff = len(new_music[i]) - 1000\n",
        "        if (diff > 0):\n",
        "          new_music[i] = new_music[i][:-diff]\n",
        "        elif (diff < 0):\n",
        "          for j in range(-diff):\n",
        "            new_music[i].append(0)\n",
        "\n",
        "    list(set(new_music.ravel()[0]))\n",
        "\n",
        "\n",
        "    #DICTIONARY unique notes --> numbers\n",
        "    unique_x = np.unique(np.asarray([new_music[i][0:1001] for i in range(26)]).reshape(-1))\n",
        "    x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
        "\n",
        "    #preparing input sequences\n",
        "    x_seq=[]\n",
        "    for i in new_music:\n",
        "        temp=[]\n",
        "        for j in i:\n",
        "            #assigning unique integer to every note\n",
        "            temp.append(x_note_to_int[str(j)])\n",
        "        x_seq.append(temp)\n",
        "\n",
        "    x_seq = np.array(x_seq)\n",
        "\n",
        "    x = []\n",
        "    window_size = 32 #increase\n",
        "    for i in tqdm(x_seq):\n",
        "        this_x_ = [] # placeholder\n",
        "        for j in range(len(i)-window_size):\n",
        "            this_x_.append(i[j:(j+window_size)])\n",
        "        x.append(this_x_)\n",
        "\n",
        "    x_arr = np.asarray(x)\n",
        "    x_arr_resh = x_arr.reshape((4, 968, 32, 1))\n",
        "\n",
        "    bigX, bigY = x_arr_resh[:, 0:967, :, :], x_arr_resh[:, 967, :, :]\n",
        "    bigY = bigY.reshape((4, 32))\n",
        "\n",
        "    return bigX, bigY, power_data, new_music\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oWPDYO0EjkiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ROCK CALM DATA\n",
        "\n",
        "def createRockCalmData(musicPath, emotionData):\n",
        "\n",
        "    #CSV EMOTION DATA\n",
        "\n",
        "    new_data = emotionData.groupby('track id').mean().round(0)\n",
        "    new_data['track_id'] = new_data.index\n",
        "    final_data = new_data[['track_id', 'calmness', 'power']]\n",
        "\n",
        "    calm_data_list = []\n",
        "\n",
        "    for x in final_data.index:\n",
        "        if final_data['calmness'][x] == 1.0:\n",
        "            calm_data_list.append(x)\n",
        "\n",
        "    calm_data = pd.DataFrame(calm_data_list)\n",
        "    calm_data.to_csv('final_calm_data_for_annotation.csv')\n",
        "\n",
        "    #MUSIC DATA\n",
        "\n",
        "    #read all the filenames\n",
        "    files=[i for i in os.listdir(musicPath) ]\n",
        "\n",
        "    calmFiles = []\n",
        "\n",
        "    for i in files:\n",
        "        if i[0:2] != \".i\":\n",
        "          if int(i[0:2]) in calm_data_list:\n",
        "            print(i)\n",
        "            calmFiles.append(i)\n",
        "\n",
        "    #reading each midi file\n",
        "    notes_array = np.array([read_midi(musicPath+i) for i in calmFiles])\n",
        "\n",
        "    #converting 2D array into 1D array\n",
        "    notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "    #No. of unique notes\n",
        "    unique_notes = list(set(notes_))\n",
        "\n",
        "    freq = dict(Counter(notes_))\n",
        "    frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "\n",
        "    #concatenating all 100 audio files\n",
        "    new_music=[]\n",
        "\n",
        "    for notes in notes_array:\n",
        "        temp=[]\n",
        "        for note_ in notes:\n",
        "            if note_ in frequent_notes:\n",
        "                temp.append(note_)\n",
        "        new_music.append(temp)\n",
        "\n",
        "    new_music = np.array(new_music)\n",
        "\n",
        "    #cutting all files to 1000\n",
        "    diff = 0\n",
        "\n",
        "    for i in range(26):\n",
        "        diff = len(new_music[i]) - 1000\n",
        "        if (diff > 0):\n",
        "          new_music[i] = new_music[i][:-diff]\n",
        "        elif (diff < 0):\n",
        "          for j in range(-diff):\n",
        "            new_music[i].append(0)\n",
        "\n",
        "    list(set(new_music.ravel()[0]))\n",
        "\n",
        "\n",
        "    #DICTIONARY unique notes --> numbers\n",
        "    unique_x = np.unique(np.asarray([new_music[i][0:1001] for i in range(26)]).reshape(-1))\n",
        "    x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
        "\n",
        "    #preparing input sequences\n",
        "    x_seq=[]\n",
        "    for i in new_music:\n",
        "        temp=[]\n",
        "        for j in i:\n",
        "            #assigning unique integer to every note\n",
        "            temp.append(x_note_to_int[str(j)])\n",
        "        x_seq.append(temp)\n",
        "\n",
        "    x_seq = np.array(x_seq)\n",
        "\n",
        "    x = []\n",
        "    window_size = 32 #increase\n",
        "    for i in tqdm(x_seq):\n",
        "        this_x_ = [] # placeholder\n",
        "        for j in range(len(i)-window_size):\n",
        "            this_x_.append(i[j:(j+window_size)])\n",
        "        x.append(this_x_)\n",
        "\n",
        "    x_arr = np.asarray(x)\n",
        "    x_arr_resh = x_arr.reshape((12, 968, 32, 1))\n",
        "\n",
        "    bigX, bigY = x_arr_resh[:, 0:967, :, :], x_arr_resh[:, 967, :, :]\n",
        "    bigY = bigY.reshape((12, 32))\n",
        "\n",
        "    return bigX, bigY, calm_data, new_music\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6eNtEt8uEtY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ROCK POWER DATA\n",
        "\n",
        "def createRockPowerData(musicPath, emotionData):\n",
        "\n",
        "    #CSV EMOTION DATA\n",
        "\n",
        "    new_data = emotionData.groupby('track id').mean().round(0)\n",
        "    new_data['track_id'] = new_data.index\n",
        "    final_data = new_data[['track_id', 'calmness', 'power']]\n",
        "\n",
        "    power_data_list = []\n",
        "\n",
        "    for x in final_data.index:\n",
        "        if final_data['power'][x] == 1.0:\n",
        "            power_data_list.append(x)\n",
        "\n",
        "    power_data = pd.DataFrame(power_data_list)\n",
        "    power_data.to_csv('final_power_data_for_annotation.csv')\n",
        "\n",
        "    #MUSIC DATA\n",
        "\n",
        "    #read all the filenames\n",
        "    files=[i for i in os.listdir(musicPath) ]\n",
        "\n",
        "    powerFiles = []\n",
        "\n",
        "    for i in files:\n",
        "        if i[0:2] != \".i\":\n",
        "          if int(i[0:2]) in power_data_list:\n",
        "            print(i)\n",
        "            powerFiles.append(i)\n",
        "\n",
        "    #reading each midi file\n",
        "    notes_array = np.array([read_midi(musicPath+i) for i in powerFiles])\n",
        "\n",
        "    #converting 2D array into 1D array\n",
        "    notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "    #No. of unique notes\n",
        "    unique_notes = list(set(notes_))\n",
        "\n",
        "    freq = dict(Counter(notes_))\n",
        "    frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "\n",
        "    #concatenating all 100 audio files\n",
        "    new_music=[]\n",
        "\n",
        "    for notes in notes_array:\n",
        "        temp=[]\n",
        "        for note_ in notes:\n",
        "            if note_ in frequent_notes:\n",
        "                temp.append(note_)\n",
        "        new_music.append(temp)\n",
        "\n",
        "    new_music = np.array(new_music)\n",
        "\n",
        "    #cutting all files to 1000\n",
        "    diff = 0\n",
        "\n",
        "    for i in range(26):\n",
        "        diff = len(new_music[i]) - 1000\n",
        "        if (diff > 0):\n",
        "          new_music[i] = new_music[i][:-diff]\n",
        "        elif (diff < 0):\n",
        "          for j in range(-diff):\n",
        "            new_music[i].append(0)\n",
        "\n",
        "    list(set(new_music.ravel()[0]))\n",
        "\n",
        "\n",
        "    #DICTIONARY unique notes --> numbers\n",
        "    unique_x = np.unique(np.asarray([new_music[i][0:1001] for i in range(26)]).reshape(-1))\n",
        "    x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
        "\n",
        "    #preparing input sequences\n",
        "    x_seq=[]\n",
        "    for i in new_music:\n",
        "        temp=[]\n",
        "        for j in i:\n",
        "            #assigning unique integer to every note\n",
        "            temp.append(x_note_to_int[str(j)])\n",
        "        x_seq.append(temp)\n",
        "\n",
        "    x_seq = np.array(x_seq)\n",
        "\n",
        "    x = []\n",
        "    window_size = 32 #increase\n",
        "    for i in tqdm(x_seq):\n",
        "        this_x_ = [] # placeholder\n",
        "        for j in range(len(i)-window_size):\n",
        "            this_x_.append(i[j:(j+window_size)])\n",
        "        x.append(this_x_)\n",
        "\n",
        "    x_arr = np.asarray(x)\n",
        "    x_arr_resh = x_arr.reshape((6, 968, 32, 1))\n",
        "\n",
        "    bigX, bigY = x_arr_resh[:, 0:967, :, :], x_arr_resh[:, 967, :, :]\n",
        "    bigY = bigY.reshape((6, 32))\n",
        "\n",
        "    return bigX, bigY, power_data, new_music\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "hBjl0VP0EtZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ELECTRONIC CALM DATA\n",
        "\n",
        "def createElectronicCalmData(musicPath, emotionData):\n",
        "\n",
        "    #CSV EMOTION DATA\n",
        "\n",
        "    new_data = emotionData.groupby('track id').mean().round(0)\n",
        "    new_data['track_id'] = new_data.index\n",
        "    final_data = new_data[['track_id', 'calmness', 'power']]\n",
        "\n",
        "    calm_data_list = []\n",
        "\n",
        "    for x in final_data.index:\n",
        "        if final_data['calmness'][x] == 1.0:\n",
        "            calm_data_list.append(x)\n",
        "\n",
        "    calm_data = pd.DataFrame(calm_data_list)\n",
        "    calm_data.to_csv('final_calm_data_for_annotation.csv')\n",
        "\n",
        "    #MUSIC DATA\n",
        "\n",
        "    #read all the filenames\n",
        "    files=[i for i in os.listdir(musicPath) ]\n",
        "\n",
        "    calmFiles = []\n",
        "\n",
        "    for i in files:\n",
        "        if i[0:2] != \".i\":\n",
        "          if int(i[0:2]) in calm_data_list:\n",
        "            print(i)\n",
        "            calmFiles.append(i)\n",
        "\n",
        "    #reading each midi file\n",
        "    notes_array = np.array([read_midi(musicPath+i) for i in calmFiles])\n",
        "\n",
        "    #converting 2D array into 1D array\n",
        "    notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "    #No. of unique notes\n",
        "    unique_notes = list(set(notes_))\n",
        "\n",
        "    freq = dict(Counter(notes_))\n",
        "    frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "\n",
        "    #concatenating all 100 audio files\n",
        "    new_music=[]\n",
        "\n",
        "    for notes in notes_array:\n",
        "        temp=[]\n",
        "        for note_ in notes:\n",
        "            if note_ in frequent_notes:\n",
        "                temp.append(note_)\n",
        "        new_music.append(temp)\n",
        "\n",
        "    new_music = np.array(new_music)\n",
        "\n",
        "    #cutting all files to 1000\n",
        "    diff = 0\n",
        "\n",
        "    for i in range(26):\n",
        "        diff = len(new_music[i]) - 1000\n",
        "        if (diff > 0):\n",
        "          new_music[i] = new_music[i][:-diff]\n",
        "        elif (diff < 0):\n",
        "          for j in range(-diff):\n",
        "            new_music[i].append(0)\n",
        "\n",
        "    list(set(new_music.ravel()[0]))\n",
        "\n",
        "\n",
        "    #DICTIONARY unique notes --> numbers\n",
        "    unique_x = np.unique(np.asarray([new_music[i][0:1001] for i in range(26)]).reshape(-1))\n",
        "    x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
        "\n",
        "    #preparing input sequences\n",
        "    x_seq=[]\n",
        "    for i in new_music:\n",
        "        temp=[]\n",
        "        for j in i:\n",
        "            #assigning unique integer to every note\n",
        "            temp.append(x_note_to_int[str(j)])\n",
        "        x_seq.append(temp)\n",
        "\n",
        "    x_seq = np.array(x_seq)\n",
        "\n",
        "    x = []\n",
        "    window_size = 32 #increase\n",
        "    for i in tqdm(x_seq):\n",
        "        this_x_ = [] # placeholder\n",
        "        for j in range(len(i)-window_size):\n",
        "            this_x_.append(i[j:(j+window_size)])\n",
        "        x.append(this_x_)\n",
        "\n",
        "    x_arr = np.asarray(x)\n",
        "    x_arr_resh = x_arr.reshape((22, 968, 32, 1))\n",
        "\n",
        "    bigX, bigY = x_arr_resh[:, 0:967, :, :], x_arr_resh[:, 967, :, :]\n",
        "    bigY = bigY.reshape((22, 32))\n",
        "\n",
        "    return bigX, bigY, calm_data, new_music\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZcipFW7fEt9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ELECTRONIC POWER DATA\n",
        "\n",
        "def createElectronicPowerData(musicPath, emotionData):\n",
        "\n",
        "    #CSV EMOTION DATA\n",
        "\n",
        "    new_data = emotionData.groupby('track id').mean().round(0)\n",
        "    new_data['track_id'] = new_data.index\n",
        "    final_data = new_data[['track_id', 'calmness', 'power']]\n",
        "\n",
        "    power_data_list = []\n",
        "\n",
        "    for x in final_data.index:\n",
        "        if final_data['power'][x] == 1.0:\n",
        "            power_data_list.append(x)\n",
        "\n",
        "    power_data = pd.DataFrame(power_data_list)\n",
        "    power_data.to_csv('final_power_data_for_annotation.csv')\n",
        "\n",
        "    #MUSIC DATA\n",
        "\n",
        "    #read all the filenames\n",
        "    files=[i for i in os.listdir(musicPath) ]\n",
        "\n",
        "    powerFiles = []\n",
        "\n",
        "    for i in files:\n",
        "        if i[0:2] != \".i\":\n",
        "          if int(i[0:2]) in power_data_list:\n",
        "            print(i)\n",
        "            powerFiles.append(i)\n",
        "\n",
        "    #reading each midi file\n",
        "    notes_array = np.array([read_midi(musicPath+i) for i in powerFiles])\n",
        "\n",
        "    #converting 2D array into 1D array\n",
        "    notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "    #No. of unique notes\n",
        "    unique_notes = list(set(notes_))\n",
        "\n",
        "    freq = dict(Counter(notes_))\n",
        "    frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "\n",
        "    #concatenating all 100 audio files\n",
        "    new_music=[]\n",
        "\n",
        "    for notes in notes_array:\n",
        "        temp=[]\n",
        "        for note_ in notes:\n",
        "            if note_ in frequent_notes:\n",
        "                temp.append(note_)\n",
        "        new_music.append(temp)\n",
        "\n",
        "    new_music = np.array(new_music)\n",
        "\n",
        "    #cutting all files to 1000\n",
        "    diff = 0\n",
        "\n",
        "    for i in range(26):\n",
        "        diff = len(new_music[i]) - 1000\n",
        "        if (diff > 0):\n",
        "          new_music[i] = new_music[i][:-diff]\n",
        "        elif (diff < 0):\n",
        "          for j in range(-diff):\n",
        "            new_music[i].append(0)\n",
        "\n",
        "    list(set(new_music.ravel()[0]))\n",
        "\n",
        "\n",
        "    #DICTIONARY unique notes --> numbers\n",
        "    unique_x = np.unique(np.asarray([new_music[i][0:1001] for i in range(26)]).reshape(-1))\n",
        "    x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
        "\n",
        "    #preparing input sequences\n",
        "    x_seq=[]\n",
        "    for i in new_music:\n",
        "        temp=[]\n",
        "        for j in i:\n",
        "            #assigning unique integer to every note\n",
        "            temp.append(x_note_to_int[str(j)])\n",
        "        x_seq.append(temp)\n",
        "\n",
        "    x_seq = np.array(x_seq)\n",
        "\n",
        "    x = []\n",
        "    window_size = 32 #increase\n",
        "    for i in tqdm(x_seq):\n",
        "        this_x_ = [] # placeholder\n",
        "        for j in range(len(i)-window_size):\n",
        "            this_x_.append(i[j:(j+window_size)])\n",
        "        x.append(this_x_)\n",
        "\n",
        "    x_arr = np.asarray(x)\n",
        "    x_arr_resh = x_arr.reshape((9, 968, 32, 1))\n",
        "\n",
        "    bigX, bigY = x_arr_resh[:, 0:967, :, :], x_arr_resh[:, 967, :, :]\n",
        "    bigY = bigY.reshape((9, 32))\n",
        "\n",
        "    return bigX, bigY, power_data, new_music\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "H55GASwjEt9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title POP CALM DATA\n",
        "\n",
        "def createPopCalmData(musicPath, emotionData):\n",
        "\n",
        "    #CSV EMOTION DATA\n",
        "\n",
        "    new_data = emotionData.groupby('track id').mean().round(0)\n",
        "    new_data['track_id'] = new_data.index\n",
        "    final_data = new_data[['track_id', 'calmness', 'power']]\n",
        "\n",
        "    calm_data_list = []\n",
        "\n",
        "    for x in final_data.index:\n",
        "        if final_data['calmness'][x] == 1.0:\n",
        "            calm_data_list.append(x)\n",
        "\n",
        "    calm_data = pd.DataFrame(calm_data_list)\n",
        "    calm_data.to_csv('final_calm_data_for_annotation.csv')\n",
        "\n",
        "    #MUSIC DATA\n",
        "\n",
        "    #read all the filenames\n",
        "    files=[i for i in os.listdir(musicPath) ]\n",
        "\n",
        "    calmFiles = []\n",
        "\n",
        "    for i in files:\n",
        "        if i[0:2] != \".i\":\n",
        "          if int(i[0:2]) in calm_data_list:\n",
        "            print(i)\n",
        "            calmFiles.append(i)\n",
        "\n",
        "    #reading each midi file\n",
        "    notes_array = np.array([read_midi(musicPath+i) for i in calmFiles])\n",
        "\n",
        "    #converting 2D array into 1D array\n",
        "    notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "    #No. of unique notes\n",
        "    unique_notes = list(set(notes_))\n",
        "\n",
        "    freq = dict(Counter(notes_))\n",
        "    frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "\n",
        "    #concatenating all 100 audio files\n",
        "    new_music=[]\n",
        "\n",
        "    for notes in notes_array:\n",
        "        temp=[]\n",
        "        for note_ in notes:\n",
        "            if note_ in frequent_notes:\n",
        "                temp.append(note_)\n",
        "        new_music.append(temp)\n",
        "\n",
        "    new_music = np.array(new_music)\n",
        "\n",
        "    #cutting all files to 1000\n",
        "    diff = 0\n",
        "\n",
        "    for i in range(26):\n",
        "        diff = len(new_music[i]) - 1000\n",
        "        if (diff > 0):\n",
        "          new_music[i] = new_music[i][:-diff]\n",
        "        elif (diff < 0):\n",
        "          for j in range(-diff):\n",
        "            new_music[i].append(0)\n",
        "\n",
        "    list(set(new_music.ravel()[0]))\n",
        "\n",
        "\n",
        "    #DICTIONARY unique notes --> numbers\n",
        "    unique_x = np.unique(np.asarray([new_music[i][0:1001] for i in range(26)]).reshape(-1))\n",
        "    x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
        "\n",
        "    #preparing input sequences\n",
        "    x_seq=[]\n",
        "    for i in new_music:\n",
        "        temp=[]\n",
        "        for j in i:\n",
        "            #assigning unique integer to every note\n",
        "            temp.append(x_note_to_int[str(j)])\n",
        "        x_seq.append(temp)\n",
        "\n",
        "    x_seq = np.array(x_seq)\n",
        "\n",
        "    x = []\n",
        "    window_size = 32 #increase\n",
        "    for i in tqdm(x_seq):\n",
        "        this_x_ = [] # placeholder\n",
        "        for j in range(len(i)-window_size):\n",
        "            this_x_.append(i[j:(j+window_size)])\n",
        "        x.append(this_x_)\n",
        "\n",
        "    x_arr = np.asarray(x)\n",
        "    x_arr_resh = x_arr.reshape((19, 968, 32, 1))\n",
        "\n",
        "    bigX, bigY = x_arr_resh[:, 0:967, :, :], x_arr_resh[:, 967, :, :]\n",
        "    bigY = bigY.reshape((19, 32))\n",
        "\n",
        "    return bigX, bigY, calm_data, new_music\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7ZNjMhO5EuMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title POP POWER DATA\n",
        "\n",
        "def createPopPowerData(musicPath, emotionData):\n",
        "\n",
        "    #CSV EMOTION DATA\n",
        "\n",
        "    new_data = emotionData.groupby('track id').mean().round(0)\n",
        "    new_data['track_id'] = new_data.index\n",
        "    final_data = new_data[['track_id', 'calmness', 'power']]\n",
        "\n",
        "    power_data_list = []\n",
        "\n",
        "    for x in final_data.index:\n",
        "        if final_data['power'][x] == 1.0:\n",
        "            power_data_list.append(x)\n",
        "\n",
        "    power_data = pd.DataFrame(power_data_list)\n",
        "    power_data.to_csv('final_power_data_for_annotation.csv')\n",
        "\n",
        "    #MUSIC DATA\n",
        "\n",
        "    #read all the filenames\n",
        "    files=[i for i in os.listdir(musicPath) ]\n",
        "\n",
        "    powerFiles = []\n",
        "\n",
        "    for i in files:\n",
        "        if i[0:2] != \".i\":\n",
        "          if int(i[0:2]) in power_data_list:\n",
        "            print(i)\n",
        "            powerFiles.append(i)\n",
        "\n",
        "    #reading each midi file\n",
        "    notes_array = np.array([read_midi(musicPath+i) for i in powerFiles])\n",
        "\n",
        "    #converting 2D array into 1D array\n",
        "    notes_ = [element for note_ in notes_array for element in note_]\n",
        "\n",
        "    #No. of unique notes\n",
        "    unique_notes = list(set(notes_))\n",
        "\n",
        "    freq = dict(Counter(notes_))\n",
        "    frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
        "\n",
        "    #concatenating all 100 audio files\n",
        "    new_music=[]\n",
        "\n",
        "    for notes in notes_array:\n",
        "        temp=[]\n",
        "        for note_ in notes:\n",
        "            if note_ in frequent_notes:\n",
        "                temp.append(note_)\n",
        "        new_music.append(temp)\n",
        "\n",
        "    new_music = np.array(new_music)\n",
        "\n",
        "    #cutting all files to 1000\n",
        "    diff = 0\n",
        "\n",
        "    for i in range(26):\n",
        "        diff = len(new_music[i]) - 1000\n",
        "        if (diff > 0):\n",
        "          new_music[i] = new_music[i][:-diff]\n",
        "        elif (diff < 0):\n",
        "          for j in range(-diff):\n",
        "            new_music[i].append(0)\n",
        "\n",
        "    list(set(new_music.ravel()[0]))\n",
        "\n",
        "\n",
        "    #DICTIONARY unique notes --> numbers\n",
        "    unique_x = np.unique(np.asarray([new_music[i][0:1001] for i in range(26)]).reshape(-1))\n",
        "    x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))\n",
        "\n",
        "    #preparing input sequences\n",
        "    x_seq=[]\n",
        "    for i in new_music:\n",
        "        temp=[]\n",
        "        for j in i:\n",
        "            #assigning unique integer to every note\n",
        "            temp.append(x_note_to_int[str(j)])\n",
        "        x_seq.append(temp)\n",
        "\n",
        "    x_seq = np.array(x_seq)\n",
        "\n",
        "    x = []\n",
        "    window_size = 32 #increase\n",
        "    for i in tqdm(x_seq):\n",
        "        this_x_ = [] # placeholder\n",
        "        for j in range(len(i)-window_size):\n",
        "            this_x_.append(i[j:(j+window_size)])\n",
        "        x.append(this_x_)\n",
        "\n",
        "    x_arr = np.asarray(x)\n",
        "    x_arr_resh = x_arr.reshape((4, 968, 32, 1))\n",
        "\n",
        "    bigX, bigY = x_arr_resh[:, 0:967, :, :], x_arr_resh[:, 967, :, :]\n",
        "    bigY = bigY.reshape((4, 32))\n",
        "\n",
        "    return bigX, bigY, power_data, new_music\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gS0eyoETEuMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ypyUfUAvewM",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title BUILD MODEL\n",
        "def build_seamise_model_with_functional():\n",
        "\n",
        "    # instantiate the input Tensor\n",
        "    input_layer = tf.keras.Input(shape=(967, 32, 1))\n",
        "    input_layer_aux = tf.keras.Input(shape=[1])\n",
        "\n",
        "    # stack the layers\n",
        "    convlayer = tf.keras.layers.Conv2D(128, (10,2))(input_layer)\n",
        "    convlayer2 = tf.keras.layers.Conv2D(128, (10,2))(convlayer)\n",
        "    flatten_layer = tf.keras.layers.Flatten()(convlayer2)\n",
        "    flatten_layer_aux = tf.keras.layers.Flatten()(input_layer_aux)\n",
        "    first_dense = tf.keras.layers.Dense(128, activation=tf.nn.relu)(flatten_layer)\n",
        "    second_dense = tf.keras.layers.Dense(128, activation=tf.nn.relu)(first_dense)\n",
        "    third_dense = tf.keras.layers.Dense(128, activation=tf.nn.relu)(second_dense)\n",
        "    concate = tf.keras.layers.Concatenate()([third_dense, input_layer_aux])\n",
        "    output_layer = tf.keras.layers.Dense(32)(concate)\n",
        "\n",
        "    # declare inputs and outputs\n",
        "    func_model = tf.keras.models.Model(inputs=[input_layer, input_layer_aux], outputs=output_layer)\n",
        "\n",
        "    return func_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PREDICT\n",
        "def predict(music, emotion, model, new_music):\n",
        "    output_of_ONE_SONG = model.predict([music[0:1], emotion[0:1]])\n",
        "    intSong = list(np.abs(np.round(output_of_ONE_SONG[0]/100, )).astype(int))\n",
        "\n",
        "    unique_x = np.unique(np.asarray([new_music[i][0:1001] for i in range(26)]).reshape(-1))\n",
        "    x_int_to_note = dict((number, note_) for number, note_ in enumerate(unique_x))\n",
        "\n",
        "    print(x_int_to_note)\n",
        "    print(intSong)\n",
        "\n",
        "    length = len(x_int_to_note.keys())\n",
        "    print(\"length: \" + str(length))\n",
        "\n",
        "    predicted_notes = [x_int_to_note[i] for i in intSong]\n",
        "\n",
        "    print(\"predicted notes\")\n",
        "    print(predicted_notes)\n",
        "    print(len(predicted_notes))\n",
        "\n",
        "    predicted_notes = list(map(float, predicted_notes))\n",
        "\n",
        "    predicted_notes = np.round(predicted_notes / np.max(predicted_notes) * length).astype(int)\n",
        "\n",
        "    return predicted_notes"
      ],
      "metadata": {
        "id": "YDYe7CP1x5EI",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYzC2QHQYgcs",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title CONVERT TO MIDI\n",
        "def convert_to_midi(prediction_output):\n",
        "\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "\n",
        "        # pattern is a chord\n",
        "        if type(pattern) == int:\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "\n",
        "                cn=int(current_note)\n",
        "                new_note = note.Note(cn)\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "\n",
        "\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "\n",
        "        # pattern is a note\n",
        "        else:\n",
        "\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 1\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "    midi_stream.write('midi', fp='music.mid')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CLASSICAL CALM MODEL\n",
        "def classicalCalm():\n",
        "\n",
        "    musicPath =\"/content/drive/MyDrive/classicalmidi/\"\n",
        "\n",
        "    csvPath = '/content/drive/MyDrive/emotifyannotationsdata'\n",
        "    os.chdir(csvPath)\n",
        "\n",
        "    annot = pd.read_csv('classical.csv')\n",
        "    annot.columns = ['track id', 'genre', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked', 'age', 'gender', 'mother tongue']\n",
        "    annot_subset = annot[['track id', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked']]\n",
        "\n",
        "    print (\"create final data\")\n",
        "\n",
        "    bigX, bigY, calm_data, new_music = createClassicalCalmData(musicPath, annot_subset)\n",
        "\n",
        "    print (\"finished loading data\")\n",
        "\n",
        "    #CREATE MODEL\n",
        "\n",
        "    print (\"create model\")\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = build_seamise_model_with_functional()\n",
        "    tf.keras.utils.plot_model(model)\n",
        "    keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    opt = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.0005,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False,\n",
        "        name='adam',\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mae')\n",
        "\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        history = model.fit(\n",
        "            x=[bigX, calm_data],\n",
        "            y=bigY,\n",
        "            validation_split = 0.2,\n",
        "            batch_size=3,\n",
        "            epochs=2,\n",
        "            callbacks=[callback])\n",
        "\n",
        "    model.save('/content/drive/MyDrive/emotifymusicMD/classical_calm.h5')\n",
        "\n",
        "    predicted_notes = predict(bigX, calm_data, model, new_music)\n",
        "\n",
        "    print (predicted_notes)\n",
        "\n",
        "    print (\"converting output to midi\")\n",
        "    convert_to_midi(predicted_notes)\n",
        "\n",
        "    !apt install fluidsynth\n",
        "    !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "    !fluidsynth -ni font.sf2 music.mid -F output.wav -r 44100\n",
        "    from IPython.display import Audio\n",
        "    return Audio('output.wav')"
      ],
      "metadata": {
        "id": "pfBnawLobrKk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CLASSICAL POWER MODEL\n",
        "def classicalPower():\n",
        "\n",
        "    musicPath =\"/content/drive/MyDrive/classicalmidi/\"\n",
        "\n",
        "    csvPath = '/content/drive/MyDrive/emotifyannotationsdata/classical.csv'\n",
        "    os.chdir(csvPath)\n",
        "\n",
        "    annot = pd.read_csv('emotifyannotationsdata.csv')\n",
        "    annot.columns = ['track id', 'genre', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked', 'age', 'gender', 'mother tongue']\n",
        "    annot_subset = annot[['track id', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked']]\n",
        "\n",
        "    print (\"create final data\")\n",
        "\n",
        "    bigX, bigY, power_data, new_music = createClassicalPowerData(musicPath, annot_subset)\n",
        "\n",
        "    print (\"finished loading data\")\n",
        "\n",
        "    #CREATE MODEL\n",
        "\n",
        "    print (\"create model\")\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = build_seamise_model_with_functional()\n",
        "    tf.keras.utils.plot_model(model)\n",
        "    keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    opt = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.0005,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False,\n",
        "        name='adam',\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mae')\n",
        "\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        history = model.fit(\n",
        "            x=[bigX, power_data],\n",
        "            y=bigY,\n",
        "            validation_split = 0.2,\n",
        "            batch_size=3,\n",
        "            epochs=2,\n",
        "            callbacks=[callback])\n",
        "\n",
        "    model.save('/content/drive/MyDrive/emotifymusicMD/classical_power.h5')\n",
        "\n",
        "    predicted_notes = predict(bigX, power_data, model, new_music)\n",
        "\n",
        "    print (\"converting output to midi\")\n",
        "    convert_to_midi(predicted_notes)\n",
        "\n",
        "    !apt install fluidsynth\n",
        "    !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "    !fluidsynth -ni font.sf2 music.mid -F output.wav -r 44100\n",
        "    from IPython.display import Audio\n",
        "    return Audio('output.wav')"
      ],
      "metadata": {
        "id": "jjZDka0Mq-ci",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ROCK CALM MODEL\n",
        "def rockCalm():\n",
        "\n",
        "    musicPath =\"/content/drive/MyDrive/rockmidi/\"\n",
        "\n",
        "    csvPath = '/content/drive/MyDrive/emotifyannotationsdata'\n",
        "    os.chdir(csvPath)\n",
        "\n",
        "    annot = pd.read_csv('rock.csv')\n",
        "    annot.columns = ['track id', 'genre', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked', 'age', 'gender', 'mother tongue']\n",
        "    annot_subset = annot[['track id', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked']]\n",
        "\n",
        "    print (\"create final data\")\n",
        "\n",
        "    bigX, bigY, calm_data, new_music = createRockCalmData(musicPath, annot_subset)\n",
        "\n",
        "    print (\"finished loading data\")\n",
        "\n",
        "    #CREATE MODEL\n",
        "\n",
        "    print (\"create model\")\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = build_seamise_model_with_functional()\n",
        "    tf.keras.utils.plot_model(model)\n",
        "    keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    opt = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.0005,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False,\n",
        "        name='adam',\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mae')\n",
        "\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        history = model.fit(\n",
        "            x=[bigX, calm_data],\n",
        "            y=bigY,\n",
        "            validation_split = 0.2,\n",
        "            batch_size=3,\n",
        "            epochs=2,\n",
        "            callbacks=[callback])\n",
        "\n",
        "    model.save('/content/drive/MyDrive/emotifymusicMD/rock_calm.h5')\n",
        "\n",
        "    predicted_notes = predict(bigX, calm_data, model, new_music)\n",
        "\n",
        "    print (\"converting output to midi\")\n",
        "    convert_to_midi(predicted_notes)\n",
        "\n",
        "    !apt install fluidsynth\n",
        "    !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "    !fluidsynth -ni font.sf2 music.mid -F output.wav -r 44100\n",
        "    from IPython.display import Audio\n",
        "    return Audio('output.wav')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "8G1p8iwFq-nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ROCK POWER MODEL\n",
        "def rockPower():\n",
        "\n",
        "    musicPath =\"/content/drive/MyDrive/rockmidi/\"\n",
        "\n",
        "    csvPath = '/content/drive/MyDrive/emotifyannotationsdata/rock.csv'\n",
        "    os.chdir(csvPath)\n",
        "\n",
        "    annot = pd.read_csv('emotifyannotationsdata.csv')\n",
        "    annot.columns = ['track id', 'genre', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked', 'age', 'gender', 'mother tongue']\n",
        "    annot_subset = annot[['track id', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked']]\n",
        "\n",
        "    print (\"create final data\")\n",
        "\n",
        "    bigX, bigY, power_data, new_music = createRockPowerData(musicPath, annot_subset)\n",
        "\n",
        "    print (\"finished loading data\")\n",
        "\n",
        "    #CREATE MODEL\n",
        "\n",
        "    print (\"create model\")\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = build_seamise_model_with_functional()\n",
        "    tf.keras.utils.plot_model(model)\n",
        "    keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    opt = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.0005,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False,\n",
        "        name='adam',\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mae')\n",
        "\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        history = model.fit(\n",
        "            x=[bigX, power_data],\n",
        "            y=bigY,\n",
        "            validation_split = 0.2,\n",
        "            batch_size=3,\n",
        "            epochs=2,\n",
        "            callbacks=[callback])\n",
        "\n",
        "    model.save('/content/drive/MyDrive/emotifymusicMD/rock_power.h5')\n",
        "\n",
        "    predicted_notes = predict(bigX, power_data, model, new_music)\n",
        "\n",
        "    print (\"converting output to midi\")\n",
        "    convert_to_midi(predicted_notes)\n",
        "\n",
        "    !apt install fluidsynth\n",
        "    !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "    !fluidsynth -ni font.sf2 music.mid -F output.wav -r 44100\n",
        "    from IPython.display import Audio\n",
        "    return Audio('output.wav')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NDDFGI7tq-4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ELECTRONIC CALM MODEL\n",
        "def electronicCalm():\n",
        "\n",
        "    musicPath =\"/content/drive/MyDrive/electronicmidi/\"\n",
        "\n",
        "    csvPath = '/content/drive/MyDrive/emotifyannotationsdata/electronic.csv'\n",
        "    os.chdir(csvPath)\n",
        "\n",
        "    annot = pd.read_csv('emotifyannotationsdata.csv')\n",
        "    annot.columns = ['track id', 'genre', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked', 'age', 'gender', 'mother tongue']\n",
        "    annot_subset = annot[['track id', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked']]\n",
        "\n",
        "    print (\"create final data\")\n",
        "\n",
        "    bigX, bigY, calm_data, new_music = createElectronicCalmData(musicPath, annot_subset)\n",
        "\n",
        "    print (\"finished loading data\")\n",
        "\n",
        "    #CREATE MODEL\n",
        "\n",
        "    print (\"create model\")\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = build_seamise_model_with_functional()\n",
        "    tf.keras.utils.plot_model(model)\n",
        "    keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    opt = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.0005,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False,\n",
        "        name='adam',\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mae')\n",
        "\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        history = model.fit(\n",
        "            x=[bigX, calm_data],\n",
        "            y=bigY,\n",
        "            validation_split = 0.2,\n",
        "            batch_size=3,\n",
        "            epochs=2,\n",
        "            callbacks=[callback])\n",
        "\n",
        "    model.save('/content/drive/MyDrive/emotifymusicMD/electronic_calm.h5')\n",
        "\n",
        "    predicted_notes = predict(bigX, calm_data, model, new_music)\n",
        "\n",
        "    print (\"converting output to midi\")\n",
        "    convert_to_midi(predicted_notes)\n",
        "\n",
        "    !apt install fluidsynth\n",
        "    !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "    !fluidsynth -ni font.sf2 music.mid -F output.wav -r 44100\n",
        "    from IPython.display import Audio\n",
        "    return Audio('output.wav')"
      ],
      "metadata": {
        "id": "w2UvXkgrq_Bt",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ELECTRONIC POWER MODEL\n",
        "def electronicPower():\n",
        "\n",
        "    musicPath =\"/content/drive/MyDrive/electronicmidi/\"\n",
        "\n",
        "    csvPath = '/content/drive/MyDrive/emotifyannotationsdata/electronic.csv'\n",
        "    os.chdir(csvPath)\n",
        "\n",
        "    annot = pd.read_csv('emotifyannotationsdata.csv')\n",
        "    annot.columns = ['track id', 'genre', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked', 'age', 'gender', 'mother tongue']\n",
        "    annot_subset = annot[['track id', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked']]\n",
        "\n",
        "    #CSV DATA\n",
        "\n",
        "    print (\"create final data\")\n",
        "\n",
        "    bigX, bigY, power_data, new_music = createElectronicPowerData(musicPath, annot_subset)\n",
        "\n",
        "    print (\"finished loading data\")\n",
        "\n",
        "    #CREATE MODEL\n",
        "\n",
        "    print (\"create model\")\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = build_seamise_model_with_functional()\n",
        "    tf.keras.utils.plot_model(model)\n",
        "    keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    opt = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.0005,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False,\n",
        "        name='adam',\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mae')\n",
        "\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        history = model.fit(\n",
        "            x=[bigX, power_data],\n",
        "            y=bigY,\n",
        "            validation_split = 0.2,\n",
        "            batch_size=3,\n",
        "            epochs=2,\n",
        "            callbacks=[callback])\n",
        "\n",
        "    model.save('/content/drive/MyDrive/emotifymusicMD/electronic_power.h5')\n",
        "\n",
        "    predicted_notes = predict(bigX, power_data, model, new_music)\n",
        "\n",
        "    print (\"converting output to midi\")\n",
        "    convert_to_midi(predicted_notes)\n",
        "\n",
        "    !apt install fluidsynth\n",
        "    !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "    !fluidsynth -ni font.sf2 music.mid -F output.wav -r 44100\n",
        "    from IPython.display import Audio\n",
        "    return Audio('output.wav')"
      ],
      "metadata": {
        "id": "7oVSuEnmq_H3",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title POP CALM MODEL\n",
        "def popCalm():\n",
        "\n",
        "    musicPath =\"/content/drive/MyDrive/popmidi/\"\n",
        "\n",
        "    csvPath = '/content/drive/MyDrive/emotifyannotationsdata/pop.csv'\n",
        "    os.chdir(csvPath)\n",
        "\n",
        "    annot = pd.read_csv('emotifyannotationsdata.csv')\n",
        "    annot.columns = ['track id', 'genre', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked', 'age', 'gender', 'mother tongue']\n",
        "    annot_subset = annot[['track id', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked']]\n",
        "\n",
        "    print (\"create final data\")\n",
        "\n",
        "    bigX, bigY, calm_data, new_music = createPopCalmData(musicPath, annot_subset)\n",
        "\n",
        "    print (\"finished loading data\")\n",
        "\n",
        "    #CREATE MODEL\n",
        "\n",
        "    print (\"create model\")\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = build_seamise_model_with_functional()\n",
        "    tf.keras.utils.plot_model(model)\n",
        "    keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    opt = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.0005,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False,\n",
        "        name='adam',\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mae')\n",
        "\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        history = model.fit(\n",
        "            x=[bigX, calm_data],\n",
        "            y=bigY,\n",
        "            validation_split = 0.2,\n",
        "            batch_size=3,\n",
        "            epochs=2,\n",
        "            callbacks=[callback])\n",
        "\n",
        "    model.save('/content/drive/MyDrive/emotifymusicMD/pop_calm.h5')\n",
        "\n",
        "    predicted_notes = predict(bigX, calm_data, model, new_music)\n",
        "\n",
        "    print (\"converting output to midi\")\n",
        "    convert_to_midi(predicted_notes)\n",
        "\n",
        "    !apt install fluidsynth\n",
        "    !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "    !fluidsynth -ni font.sf2 music.mid -F output.wav -r 44100\n",
        "    from IPython.display import Audio\n",
        "    return Audio('output.wav')"
      ],
      "metadata": {
        "id": "NIhrD4ggq_QG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title POP POWER MODEL\n",
        "def popPower():\n",
        "\n",
        "    musicPath =\"/content/drive/MyDrive/popmidi/\"\n",
        "\n",
        "    csvPath = '/content/drive/MyDrive/emotifyannotationsdata/pop.csv'\n",
        "    os.chdir(csvPath)\n",
        "\n",
        "    annot = pd.read_csv('emotifyannotationsdata.csv')\n",
        "    annot.columns = ['track id', 'genre', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked', 'age', 'gender', 'mother tongue']\n",
        "    annot_subset = annot[['track id', 'amazement', 'solemnity', 'tenderness',\n",
        "          'nostalgia', 'calmness', 'power', 'joyful_activation', 'tension',\n",
        "          'sadness', 'mood', 'liked', 'disliked']]\n",
        "\n",
        "    print (\"create final data\")\n",
        "\n",
        "    bigX, bigY, power_data, new_music = createPopPowerData(musicPath, annot_subset)\n",
        "\n",
        "    print (\"finished loading data\")\n",
        "\n",
        "    #CREATE MODEL\n",
        "\n",
        "    print (\"create model\")\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = build_seamise_model_with_functional()\n",
        "    tf.keras.utils.plot_model(model)\n",
        "    keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    opt = tf.keras.optimizers.Adam(\n",
        "        learning_rate=0.0005,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-07,\n",
        "        amsgrad=False,\n",
        "        name='adam',\n",
        "    )\n",
        "\n",
        "    model.compile(optimizer=opt, loss='mae')\n",
        "\n",
        "    with tf.device('/device:GPU:0'):\n",
        "        history = model.fit(\n",
        "            x=[bigX, power_data],\n",
        "            y=bigY,\n",
        "            validation_split = 0.2,\n",
        "            batch_size=3,\n",
        "            epochs=2,\n",
        "            callbacks=[callback])\n",
        "\n",
        "    model.save('/content/drive/MyDrive/emotifymusicMD/pop_power.h5')\n",
        "\n",
        "    predicted_notes = predict(bigX, power_data, model, new_music)\n",
        "\n",
        "    print (\"converting output to midi\")\n",
        "    convert_to_midi(predicted_notes)\n",
        "\n",
        "    !apt install fluidsynth\n",
        "    !cp /usr/share/sounds/sf2/FluidR3_GM.sf2 ./font.sf2\n",
        "    !fluidsynth -ni font.sf2 music.mid -F output.wav -r 44100\n",
        "    from IPython.display import Audio\n",
        "    return Audio('output.wav')"
      ],
      "metadata": {
        "id": "oCr80R6-q_Yi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title MUSIC GENERATION\n",
        "def music_generation():\n",
        "\n",
        "    #PREDICT\n",
        "    if Genre == 'classical' and Goal == 'relax':\n",
        "        return classicalCalm() #26\n",
        "    elif Genre == 'classical' and Goal == 'energize':\n",
        "        return classicalPower() #4\n",
        "    elif Genre == 'rock' and Goal == 'relax':\n",
        "        return rockCalm() #12\n",
        "    elif Genre == 'rock' and Goal == 'energize':\n",
        "        return rockPower() #6\n",
        "    elif Genre == 'electronic' and Goal == 'relax':\n",
        "        return electronicCalm() #22\n",
        "    elif Genre == 'electronic' and Goal == 'energize':\n",
        "        return electronicPower() #9\n",
        "    elif Genre == 'pop' and Goal == 'relax':\n",
        "        return popCalm() #19\n",
        "    elif Genre == 'pop' and Goal == 'energize':\n",
        "        return popPower() #4\n",
        "    else:\n",
        "        print(\"invalid input\")"
      ],
      "metadata": {
        "id": "MOLuA8U365gF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# App"
      ],
      "metadata": {
        "id": "sJ73_2-GbtWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Genre = 'classical' #@param [\"classical\", \"rock\", \"electronic\", \"pop\"]\n",
        "Goal = 'relax' #@param [\"relax\", \"energize\"]"
      ],
      "metadata": {
        "id": "eCeAjyipbt65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "music_generation()"
      ],
      "metadata": {
        "id": "JN9zm_O9b9Is"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}